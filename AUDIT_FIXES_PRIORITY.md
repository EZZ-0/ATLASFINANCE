# üî• CRITICAL AUDIT FIXES - PRIORITY ORDER

**Date:** December 1, 2025  
**Audit Source:** Third-party IC report analysis  
**Status:** In Progress

---

## üö® CRITICAL ISSUES (Fix Immediately)

### 1. **Metric Inconsistencies - P/E and D/E Ratios**

**Problem:**
- Engine shows P/E = 11.2x, but market sources show different values
- D/E = 2.47x doesn't match external sources
- Root cause: Mixing TTM financial statement data with current market price

**Why It Matters:**
- Users make investment decisions based on these ratios
- Stale data = wrong valuation assessments
- Can lead to significant financial mistakes

**Fix Required:**
```python
# Current (WRONG):
# Uses latest annual statement + current price
eps = net_income / shares_outstanding  # Annual data
pe_ratio = current_price / eps  # Current price

# Should Be (RIGHT):
# Option A: Use TTM (Trailing Twelve Months)
# Option B: Add timestamp + source label
# Option C: Pull market-calculated P/E from yfinance directly
```

**Implementation:**
1. Add `trailingPE` from yfinance info dict (already calculated by Yahoo)
2. Compare our calculated P/E vs market P/E
3. Display both with timestamps and flag discrepancies
4. Add confidence score based on data freshness

**Code Location:** `usa_backend.py` lines 668-673

---

### 2. **No Metric Provenance/Timestamps**

**Problem:**
- Every metric lacks:
  - Data source (SEC 10-K vs Yahoo vs calculated)
  - Extraction timestamp
  - Period covered (Q4 2024 vs TTM)
  - Confidence score

**Why It Matters:**
- Can't verify accuracy
- Can't debug when numbers are wrong
- Can't explain to users why values differ from other sources

**Fix Required:**
Create metadata for each metric:
```python
{
    "value": 11.2,
    "source": "yfinance",
    "method": "calculated",
    "timestamp": "2025-12-01T14:30:00Z",
    "period": "TTM Q3 2024",
    "confidence": 0.95,
    "raw_value": 11.234567
}
```

**Implementation:**
1. Create `MetricMetadata` dataclass
2. Wrap all extracted values with metadata
3. Display in UI with hover tooltips
4. Include in PDF exports

---

### 3. **Market Cap Shows N/A**

**Problem:**
- Basic field failing silently
- Shows "N/A" instead of error message

**Why It Matters:**
- Market cap is fundamental for valuation
- Silent failures hide data quality issues
- Users don't know if it's temporarily unavailable or permanently broken

**Fix Required:**
```python
# Current (WRONG):
market_cap = market_data.get('market_cap', 'N/A')

# Should Be (RIGHT):
try:
    market_cap = stock.info.get('marketCap')
    if not market_cap:
        raise ValueError("marketCap not available from yfinance")
except Exception as e:
    market_cap = {
        "value": None,
        "error": str(e),
        "status": "failed",
        "attempted_sources": ["yfinance", "calculated"]
    }
```

**Implementation:**
1. Add explicit error handling with retry logic
2. Try multiple sources (yfinance info, calculated from price √ó shares)
3. Display error message with actionable info
4. Log failures for debugging

**Code Location:** `usa_backend.py` lines 460-475

---

### 4. **Path to Target Inconsistency**

**Problem:**
- Shows "Path to Target: $25 ‚Üí $27 (+$1)" but PT is $30 (+17%)
- Numbers don't match, confusing users

**Why It Matters:**
- Destroys credibility
- Users can't trust the analysis
- Looks like a bug (and it is)

**Fix Required:**
- Audit Investment Summary generator
- Ensure all price targets use same source
- Add validation: `assert path_target == price_target`

**Code Location:** `investment_summary.py` - price target calculation

---

### 5. **"Deal-breakers: None identified" Too Risky**

**Problem:**
- Automated detection may miss critical issues:
  - Massive debt covenants
  - Pending litigation
  - Regulatory investigations
  - Going concern warnings
  - Recent restatements

**Why It Matters:**
- False negatives are dangerous in IC memos
- Need explicit checks, not implicit "nothing found"

**Fix Required:**
```python
deal_breakers = []

# Explicit checks with confidence
checks = {
    "debt_covenant_risk": check_debt_covenants(),  # confidence: 0.8
    "litigation_risk": check_sec_items(),           # confidence: 0.6
    "going_concern": check_audit_opinion(),         # confidence: 0.9
    "restatements": check_amendments()              # confidence: 0.7
}

if all(c['passed'] and c['confidence'] > 0.7 for c in checks.values()):
    result = "No deal-breakers identified (8/8 checks passed, avg confidence: 0.78)"
else:
    result = f"Potential concerns: {[k for k,v in checks.items() if not v['passed']]}"
```

---

### 6. **WACC Magic Numbers**

**Problem:**
- DCF uses hardcoded WACC defaults
- No auto-calculation from market data
- Can introduce 5-10% valuation error

**Why It Matters:**
- WACC is THE most important DCF input
- Small WACC changes = huge valuation swings
- Using wrong WACC invalidates entire DCF

**Fix Required:**
1. Calculate WACC from:
   - Cost of Equity: CAPM (Risk-free rate + Beta √ó Market premium)
   - Cost of Debt: Interest expense / Total debt
   - Weights: Market cap vs debt value
2. Display calculation breakdown
3. Allow manual override with warning
4. Show sensitivity analysis

**Code Location:** `dcf_modeling.py` - assumptions initialization

---

## ‚ö†Ô∏è HIGH PRIORITY ISSUES

### 7. **Input Validation Gaps**

**Problem:**
- TODOs note missing validation
- Can save invalid scenarios
- No range checks on extreme inputs

**Fix Required:**
```python
def validate_dcf_inputs(assumptions):
    errors = []
    
    if not (0 < assumptions.revenue_growth < 1):
        errors.append("Revenue growth must be 0-100%")
    
    if not (0.01 < assumptions.wacc < 0.5):
        errors.append("WACC must be 1-50%")
    
    if not (0 <= assumptions.terminal_growth < 0.1):
        errors.append("Terminal growth must be 0-10%")
    
    if errors:
        raise ValidationError(errors)
```

---

### 8. **No 352-Metric Canonical List**

**Problem:**
- Can't verify "352 metrics" claim
- No single source of truth
- Hard to guarantee coverage

**Fix Required:**
Create `METRIC_REGISTRY.json`:
```json
{
  "financial_statement_metrics": [
    {"key": "revenue", "source": "income_statement", "line": "Total Revenue", ...},
    {"key": "net_income", ...}
  ],
  "calculated_ratios": [...],
  "market_data": [...]
}
```

---

## üìä RECOMMENDED ADDITIONS

### 9. **Catalyst Probabilities**

Current: "Q2 2025: +$1 impact"  
Should be: "Q2 2025 Product Launch: 60% probability, +$0.60 expected impact"

### 10. **Risk Matrix Severity Calculation**

Current: Manual assessment  
Should be: Quantitative scoring with thresholds

---

## üéØ ACTION PLAN

### Phase 1 (Today - 2 hours):
1. ‚úÖ Fix Path to Target inconsistency
2. ‚úÖ Add error handling for Market Cap
3. ‚úÖ Add WACC warning message

### Phase 2 (Tomorrow - 4 hours):
4. Add metric metadata (source, timestamp, confidence)
5. Fix P/E and D/E calculation (use market values)
6. Implement deal-breaker checks

### Phase 3 (This Week - 8 hours):
7. Create metric registry
8. Add input validation
9. Implement WACC auto-calculation
10. Add catalyst probabilities

---

## üìù NOTES

**Data Source Priority:**
1. Yahoo Finance (real-time market data)
2. SEC EDGAR (historical financials)
3. Calculated (from above)

**Confidence Scoring:**
- 0.95+ = Direct API pull
- 0.80-0.94 = Calculated from reliable sources
- 0.60-0.79 = Fuzzy extraction
- <0.60 = Flag for review

**Testing:**
- Create test cases for each fix
- Compare output with MarketWatch, Macro Trends
- Document any persistent discrepancies

